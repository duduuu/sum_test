# coding=utf-8
import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np

class LSTM():

	def __init__(self, transition_system):

		self.transition_system = transition_system
		self.grammar = self.transition_system.grammar

		self.action_embed_size = 128
		self.field_embed_size = 64
		self.type_embed_size = 64
		self.hidden_size = 256

		# Embedding layers

		# embedding table of ASDL production rules (constructors), one for each ApplyConstructor action,
        # the last entry is the embedding for Reduce action
        self.production_embed = nn.Embedding(len(transition_system.grammar) + 1, action_embed_size)

		 # embedding table for ASDL fields in constructors
        self.field_embed = nn.Embedding(len(transition_system.grammar.fields), field_embed_size)

		# embedding table for ASDL types
        self.type_embed = nn.Embedding(len(transition_system.grammar.types), type_embed_size)

        nn.init.xavier_normal(self.production_embed.weight.data)
        nn.init.xavier_normal(self.field_embed.weight.data)
        nn.init.xavier_normal(self.type_embed.weight.data)

		# LSTMs
		input_dim = action_embed_size # previous action
		# parent info
		input_dim += action_embed_size
		input_dim += field_embed_size
		input_dim += type_embed_size
		input_dim += hidden_size

		input_dim += hidden_size # input feeding

		self.input_dim = input_dim

		self.lstm_cell = nn.LSTMCell(input_dim, hidden_size)

		self.lstm_cell_init = nn.Linear(hidden_size, hidden_size)

        self.production_readout_b = nn.Parameter(torch.FloatTensor(len(transition_system.grammar) + 1).zero_())

        # dropout layer
        self.dropout = nn.Dropout(dropout)

        self.new_long_tensor = torch.LongTensor
        self.new_tensor = torch.FloatTensor


	def lstm(self, batch):

		batch_size = len(batch)

		zero_action_embed = Variable(self.new_tensor(action_embed_size).zero_())

		out_vecs = []
		history_states = []

		for t in range(batch.max_action_num):
			if t == 0:
				x = Variable(self.new_tensor(batch_size, self.lstm_cell.input_size).zero_(), requires_grad=False)

				offset = action_embed_size
				offset += att_vec_size
				offset += action_embed_size
				offset += field_embed_size

				x[:, offset: offset + args.type_embed_size] = self.type_embed(Variable(self.new_long_tensor(
					[self.grammar.type2id[self.grammar.root_type] for e in batch.examples])))

			else:
				a_tm1_embeds = []
				for example in batch.examples:
					# action t -1
					if t < len(example.tgt_actions):
						a_tm1 = example.tgt_actions[t - 1]
						if isinstance(a_tm1.action, ApplyRuleAction):
                            a_tm1_embed = self.production_embed.weight[self.grammar.prod2id[a_tm1.action.production]]
						elif isinstance(a_tm1.action, ReduceAction):
                            a_tm1_embed = self.production_embed.weight[len(self.grammar)]
						else: #GetTokenAction
							a_tm1_embed = 
					else:
						a_tm1_embed = zero_action_embed

					a_tm1_embeds.append(a_tm1_embeds)

				a_tm1_embeds = torch.stack(a_tm1_embeds)

				inputs = [a_tm1_embeds] # previous action
				inputs.append(out_tm1) # input feeding
				# parent info

				parent_production_embed = self.production_embed(batch.get_frontier_prod_idx(t))
                inputs.append(parent_production_embed)
                parent_field_embed = self.field_embed(batch.get_frontier_field_idx(t))
                inputs.append(parent_field_embed)
                parent_field_type_embed = self.type_embed(batch.get_frontier_field_type_idx(t))
                inputs.append(parent_field_type_embed)

				actions_t = [e.tgt_actions[t] if t < len(e.tgt_actions) else None for e in batch.examples]
				parent_states = torch.stack([history_states[p_t][0][batch_id]
                                                 for batch_id, p_t in
                                                 enumerate(a-_t.parent_t if a_t else 0 for a_t in actions_t)])
				inputs.append(parent_states)

				x = torch.cat(inputs, dim=-1)

			h_t, cell_t = self.lstm_cell(x, h_tm1)

			out_vecs.append(h_t)
			history_states.append((h_t, cell_t))

			h_tm1 = (h_t, cell_t)
			out_tm1 = h_t

		out_vecs = torch.stack(out_vecs, dim=0)
		return out_vecs


		############

